{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Test RippleNet Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress, IntProgress\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger.py\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    def set_default_filename(self, filename):\n",
    "        self.default_filename = filename\n",
    "\n",
    "    def create_session_folder(self, path):\n",
    "        try:  \n",
    "            os.makedirs(path)\n",
    "        except OSError:  \n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "        else:  \n",
    "            print (\"\\n ===> Successfully created the directory %s \\n\" % path)\n",
    "\n",
    "    def log(self, text):\n",
    "        with open(self.default_filename, 'a') as f:\n",
    "            f.writelines(text)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class RippleNet(object):\n",
    "    def __init__(self, args, n_entity, n_relation):\n",
    "        self._parse_args(args, n_entity, n_relation)\n",
    "        self._build_inputs()\n",
    "        self._build_embeddings()\n",
    "        self._build_model()\n",
    "        self._build_loss()\n",
    "        self._build_train()\n",
    "\n",
    "    def _parse_args(self, args, n_entity, n_relation):\n",
    "        self.n_entity = n_entity\n",
    "        self.n_relation = n_relation\n",
    "        self.dim = args.dim\n",
    "        self.n_hop = args.n_hop\n",
    "        self.kge_weight = args.kge_weight\n",
    "        self.l2_weight = args.l2_weight\n",
    "        self.lr = args.lr\n",
    "        self.n_memory = args.n_memory\n",
    "        self.item_update_mode = args.item_update_mode\n",
    "        self.using_all_hops = args.using_all_hops\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        self.items = tf.placeholder(dtype=tf.int32, shape=[None], name=\"items\")\n",
    "        self.labels = tf.placeholder(dtype=tf.float64, shape=[None], name=\"labels\")\n",
    "        self.memories_h = []\n",
    "        self.memories_r = []\n",
    "        self.memories_t = []\n",
    "\n",
    "        for hop in range(self.n_hop):\n",
    "            self.memories_h.append(\n",
    "                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_h_\" + str(hop)))\n",
    "            self.memories_r.append(\n",
    "                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_r_\" + str(hop)))\n",
    "            self.memories_t.append(\n",
    "                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_t_\" + str(hop)))\n",
    "\n",
    "    def _build_embeddings(self):\n",
    "        self.entity_emb_matrix = tf.get_variable(name=\"entity_emb_matrix\", dtype=tf.float64,\n",
    "                                                 shape=[self.n_entity, self.dim],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.relation_emb_matrix = tf.get_variable(name=\"relation_emb_matrix\", dtype=tf.float64,\n",
    "                                                   shape=[self.n_relation, self.dim, self.dim],\n",
    "                                                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def _build_model(self):\n",
    "        # transformation matrix for updating item embeddings at the end of each hop\n",
    "        self.transform_matrix = tf.get_variable(name=\"transform_matrix\", shape=[self.dim, self.dim], dtype=tf.float64,\n",
    "                                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        # [batch size, dim]\n",
    "        self.item_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.items)\n",
    "\n",
    "        self.h_emb_list = []\n",
    "        self.r_emb_list = []\n",
    "        self.t_emb_list = []\n",
    "        for i in range(self.n_hop):\n",
    "            # [batch size, n_memory, dim]\n",
    "            self.h_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_h[i]))\n",
    "\n",
    "            # [batch size, n_memory, dim, dim]\n",
    "            self.r_emb_list.append(tf.nn.embedding_lookup(self.relation_emb_matrix, self.memories_r[i]))\n",
    "\n",
    "            # [batch size, n_memory, dim]\n",
    "            self.t_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_t[i]))\n",
    "\n",
    "        o_list = self._key_addressing()\n",
    "\n",
    "        self.scores = tf.squeeze(self.predict(self.item_embeddings, o_list))\n",
    "        self.scores_normalized = tf.sigmoid(self.scores)\n",
    "\n",
    "    def _key_addressing(self):\n",
    "        o_list = []\n",
    "        for hop in range(self.n_hop):\n",
    "            # [batch_size, n_memory, dim, 1]\n",
    "            h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=3)\n",
    "\n",
    "            # [batch_size, n_memory, dim]\n",
    "            Rh = tf.squeeze(tf.matmul(self.r_emb_list[hop], h_expanded), axis=3)\n",
    "\n",
    "            # [batch_size, dim, 1]\n",
    "            v = tf.expand_dims(self.item_embeddings, axis=2)\n",
    "\n",
    "            # [batch_size, n_memory]\n",
    "            probs = tf.squeeze(tf.matmul(Rh, v), axis=2)\n",
    "\n",
    "            # [batch_size, n_memory]\n",
    "            probs_normalized = tf.nn.softmax(probs)\n",
    "\n",
    "            # [batch_size, n_memory, 1]\n",
    "            probs_expanded = tf.expand_dims(probs_normalized, axis=2)\n",
    "\n",
    "            # [batch_size, dim]\n",
    "            o = tf.reduce_sum(self.t_emb_list[hop] * probs_expanded, axis=1)\n",
    "\n",
    "            self.item_embeddings = self.update_item_embedding(self.item_embeddings, o)\n",
    "            o_list.append(o)\n",
    "        return o_list\n",
    "\n",
    "    def update_item_embedding(self, item_embeddings, o):\n",
    "        if self.item_update_mode == \"replace\":\n",
    "            item_embeddings = o\n",
    "        elif self.item_update_mode == \"plus\":\n",
    "            item_embeddings = item_embeddings + o\n",
    "        elif self.item_update_mode == \"replace_transform\":\n",
    "            item_embeddings = tf.matmul(o, self.transform_matrix)\n",
    "        elif self.item_update_mode == \"plus_transform\":\n",
    "            item_embeddings = tf.matmul(item_embeddings + o, self.transform_matrix)\n",
    "        else:\n",
    "            raise Exception(\"Unknown item updating mode: \" + self.item_update_mode)\n",
    "        return item_embeddings\n",
    "\n",
    "    def predict(self, item_embeddings, o_list):\n",
    "        y = o_list[-1]\n",
    "        if self.using_all_hops:\n",
    "            for i in range(self.n_hop - 1):\n",
    "                y += o_list[i]\n",
    "\n",
    "        # [batch_size]\n",
    "        scores = tf.reduce_sum(item_embeddings * y, axis=1)\n",
    "        return scores\n",
    "\n",
    "    def _build_loss(self):\n",
    "        self.base_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))\n",
    "\n",
    "        self.kge_loss = 0\n",
    "        for hop in range(self.n_hop):\n",
    "            h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=2)\n",
    "            t_expanded = tf.expand_dims(self.t_emb_list[hop], axis=3)\n",
    "            hRt = tf.squeeze(tf.matmul(tf.matmul(h_expanded, self.r_emb_list[hop]), t_expanded))\n",
    "            self.kge_loss += tf.reduce_mean(tf.sigmoid(hRt))\n",
    "        self.kge_loss = -self.kge_weight * self.kge_loss\n",
    "\n",
    "        self.l2_loss = 0\n",
    "        for hop in range(self.n_hop):\n",
    "            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.h_emb_list[hop] * self.h_emb_list[hop]))\n",
    "            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.t_emb_list[hop] * self.t_emb_list[hop]))\n",
    "            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.r_emb_list[hop] * self.r_emb_list[hop]))\n",
    "            if self.item_update_mode == \"replace nonlinear\" or self.item_update_mode == \"plus nonlinear\":\n",
    "                self.l2_loss += tf.nn.l2_loss(self.transform_matrix)\n",
    "        self.l2_loss = self.l2_weight * self.l2_loss\n",
    "\n",
    "        self.loss = self.base_loss + self.kge_loss + self.l2_loss\n",
    "\n",
    "    def _build_train(self):\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n",
    "        gradients = [None if gradient is None else tf.clip_by_norm(gradient, clip_norm=5)\n",
    "                     for gradient in gradients]\n",
    "        self.optimizer = optimizer.apply_gradients(zip(gradients, variables))\n",
    "        '''\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer, self.loss], feed_dict)\n",
    "\n",
    "    def eval(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        acc = np.mean(np.equal(predictions, labels))\n",
    "        return auc, acc\n",
    "  \n",
    "    # ============ Custom test purpose ============\n",
    "    def custom_eval(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        acc = np.mean(np.equal(predictions, labels))\n",
    "        return auc, acc, labels, scores, predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader.py\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    train_data, eval_data, test_data, user_history_dict = load_rating(args)\n",
    "    n_entity, n_relation, kg = load_kg(args)\n",
    "    ripple_set = get_ripple_set(args, kg, user_history_dict)\n",
    "    return train_data, eval_data, test_data, n_entity, n_relation, ripple_set\n",
    "\n",
    "def load_rating(args):\n",
    "    print('reading rating file ...')\n",
    "\n",
    "    # reading rating file\n",
    "    rating_file = '../data/' + args.dataset + '/ratings_final'\n",
    "    if os.path.exists(rating_file + '.npy'):\n",
    "        rating_np = np.load(rating_file + '.npy')\n",
    "    else:\n",
    "        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int32)\n",
    "        np.save(rating_file + '.npy', rating_np)\n",
    "\n",
    "    return dataset_split(rating_np)\n",
    "\n",
    "\n",
    "def dataset_split(rating_np):\n",
    "    print('splitting dataset ...')\n",
    "\n",
    "    # train:eval:test = 6:2:2\n",
    "    eval_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "    n_ratings = rating_np.shape[0]\n",
    "\n",
    "    eval_indices = np.random.choice(n_ratings, size=int(n_ratings * eval_ratio), replace=False)\n",
    "    left = set(range(n_ratings)) - set(eval_indices)\n",
    "    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "    train_indices = list(left - set(test_indices))\n",
    "    # print(len(train_indices), len(eval_indices), len(test_i  ndices))\n",
    "\n",
    "    # traverse training data, only keeping the users with positive ratings\n",
    "    user_history_dict = dict()\n",
    "    for i in train_indices:\n",
    "        user = rating_np[i][0]\n",
    "        item = rating_np[i][1]\n",
    "        rating = rating_np[i][2]\n",
    "        if rating == 1:\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = []\n",
    "            user_history_dict[user].append(item)\n",
    "\n",
    "    train_indices = [i for i in train_indices if rating_np[i][0] in user_history_dict]\n",
    "    eval_indices = [i for i in eval_indices if rating_np[i][0] in user_history_dict]\n",
    "    test_indices = [i for i in test_indices if rating_np[i][0] in user_history_dict]\n",
    "    # print(len(train_indices), len(eval_indices), len(test_indices))\n",
    "\n",
    "    train_data = rating_np[train_indices]\n",
    "    eval_data = rating_np[eval_indices]\n",
    "    test_data = rating_np[test_indices]\n",
    "\n",
    "    return train_data, eval_data, test_data, user_history_dict\n",
    "\n",
    "\n",
    "\n",
    "def load_kg(args):\n",
    "    print('reading KG file ...')\n",
    "\n",
    "    # reading kg file\n",
    "    kg_file = '../data/' + args.dataset + '/kg_final'\n",
    "    if os.path.exists(kg_file + '.npy'):\n",
    "        kg_np = np.load(kg_file + '.npy')\n",
    "    else:\n",
    "        kg_np = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n",
    "        np.save(kg_file + '.npy', kg_np)\n",
    "\n",
    "    n_entity = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "    n_relation = len(set(kg_np[:, 1]))\n",
    "\n",
    "    kg = construct_kg(kg_np)\n",
    "\n",
    "    return n_entity, n_relation, kg\n",
    "\n",
    "\n",
    "def construct_kg(kg_np):\n",
    "    print('constructing knowledge graph ...')\n",
    "    kg = collections.defaultdict(list)\n",
    "    for head, relation, tail in kg_np:\n",
    "        kg[head].append((tail, relation))\n",
    "    return kg\n",
    "\n",
    "\n",
    "def get_ripple_set(args, kg, user_history_dict):\n",
    "    print('constructing ripple set ...')\n",
    "\n",
    "    # user -> [(hop_0_heads, hop_0_relations, hop_0_tails), (hop_1_heads, hop_1_relations, hop_1_tails), ...]\n",
    "    ripple_set = collections.defaultdict(list)\n",
    "\n",
    "    for user in tqdm(user_history_dict):\n",
    "        for h in range(args.n_hop):\n",
    "            memories_h = []\n",
    "            memories_r = []\n",
    "            memories_t = []\n",
    "\n",
    "            if h == 0:\n",
    "                tails_of_last_hop = user_history_dict[user]\n",
    "            else:\n",
    "                tails_of_last_hop = ripple_set[user][-1][2]\n",
    "\n",
    "            for entity in tails_of_last_hop:\n",
    "                for tail_and_relation in kg[entity]:\n",
    "                    memories_h.append(entity)\n",
    "                    memories_r.append(tail_and_relation[1])\n",
    "                    memories_t.append(tail_and_relation[0])\n",
    "\n",
    "            # if the current ripple set of the given user is empty, we simply copy the ripple set of the last hop here\n",
    "            # this won't happen for h = 0, because only the items that appear in the KG have been selected\n",
    "            # this only happens on 154 users in Book-Crossing dataset (since both BX dataset and the KG are sparse)\n",
    "            if len(memories_h) == 0:\n",
    "                ripple_set[user].append(ripple_set[user][-1])\n",
    "            else:\n",
    "                # sample a fixed-size 1-hop memory for each user\n",
    "                replace = len(memories_h) < args.n_memory\n",
    "                indices = np.random.choice(len(memories_h), size=args.n_memory, replace=replace)\n",
    "                memories_h = [memories_h[i] for i in indices]\n",
    "                memories_r = [memories_r[i] for i in indices]\n",
    "                memories_t = [memories_t[i] for i in indices]\n",
    "                ripple_set[user].append((memories_h, memories_r, memories_t))\n",
    "\n",
    "    return ripple_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.py\n",
    "\n",
    "\n",
    "def get_feed_dict(args, model, data, ripple_set, start, end):\n",
    "    feed_dict = dict()\n",
    "    feed_dict[model.items] = data[start:end, 1]\n",
    "    feed_dict[model.labels] = data[start:end, 2]\n",
    "    for i in range(args.n_hop):\n",
    "        feed_dict[model.memories_h[i]] = [ripple_set[user][i][0] for user in data[start:end, 0]]\n",
    "        feed_dict[model.memories_r[i]] = [ripple_set[user][i][1] for user in data[start:end, 0]]\n",
    "        feed_dict[model.memories_t[i]] = [ripple_set[user][i][2] for user in data[start:end, 0]]\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluation(sess, args, model, data, ripple_set, batch_size):\n",
    "    start = 0\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "    while start < data.shape[0]:\n",
    "        auc, acc = model.eval(sess, get_feed_dict(args, model, data, ripple_set, start, start + batch_size))\n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "        start += batch_size\n",
    "    return float(np.mean(auc_list)), float(np.mean(acc_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dataset = 'movie'\n",
    "        self.dim = 16\n",
    "        self.n_hop = 2 \n",
    "        self.kge_weight = 0.01\n",
    "        self.l2_weight = 1e-7\n",
    "        self.lr = 0.02\n",
    "        self.batch_size = 1024\n",
    "        self.n_epoch = 10\n",
    "        self.n_memory = 32\n",
    "        self.item_update_mode = 'plus_transform'\n",
    "        self.using_all_hops = True\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main.py\n",
    "\n",
    "import numpy as np\n",
    "# from Library.RippleNet.src.data_loader import load_data\n",
    "\n",
    "np.random.seed(555)\n",
    "show_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_filename = \"../data/movie/preprocessed_data_info_32\"\n",
    "\n",
    "try:\n",
    "    data_info = pickle.load(open(preprocessed_data_filename, 'rb'))\n",
    "except:\n",
    "    data_info = load_data(args)\n",
    "    pickle.dump(data_info, open(preprocessed_data_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_info[0]\n",
    "eval_data = data_info[1]\n",
    "test_data = data_info[2]\n",
    "n_entity = data_info[3]\n",
    "n_relation = data_info[4]\n",
    "ripple_set = data_info[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"1561005343.76834\"\n",
    "CHOSEN_EPOCH = 7\n",
    "\n",
    "MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n",
    "LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = RippleNet(args, n_entity, n_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../log/1561005343.76834/models/epoch_7\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "saver = tf.train.import_meta_graph(MODEL_PATH + \".meta\")\n",
    "saver.restore(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom precision at K eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 452253/452253 [00:03<00:00, 142889.79it/s]\n",
      "100%|██████████| 150737/150737 [00:01<00:00, 146852.11it/s]\n",
      "100%|██████████| 150740/150740 [00:01<00:00, 138140.06it/s]\n"
     ]
    }
   ],
   "source": [
    "truth_dict = {}\n",
    "for rating in tqdm(train_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)\n",
    "        \n",
    "for rating in tqdm(test_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)\n",
    "        \n",
    "for rating in tqdm(eval_data):\n",
    "    user_id, movie_id, score = rating\n",
    "    \n",
    "    if user_id not in truth_dict:\n",
    "        truth_dict[user_id] = []\n",
    "    \n",
    "    if score == 1:\n",
    "        truth_dict[user_id].append(movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check user - num of rating dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ns = []\n",
    "for key in truth_dict:\n",
    "    n = len(truth_dict[key])\n",
    "    ns.append(n)\n",
    "\n",
    "ns = Counter(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nscum = {}\n",
    "last = 0\n",
    "for k in ns:\n",
    "    nscum[k] = ns[k] + last\n",
    "    last = nscum[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cummulative plot\n",
    "plt.figure(figsize=(20,14))\n",
    "plt.plot(list(nscum.keys())[:50], list(nscum.values())[:50]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = 0\n",
    "ids = []\n",
    "\n",
    "for d in train_data:\n",
    "    ids.append(d[1])\n",
    "    if d[1] > 15084:\n",
    "        c += 1\n",
    "        \n",
    "for d in eval_data:\n",
    "    ids.append(d[1])\n",
    "    if d[1] > 15084:\n",
    "        c += 1\n",
    "        \n",
    "for d in test_data:\n",
    "    ids.append(d[1])\n",
    "    if d[1] > 15084:\n",
    "        c += 1\n",
    "        \n",
    "len(set(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cust_test_data(users, items):\n",
    "    dummy_value = True\n",
    "    cust_test_data = []\n",
    "    \n",
    "    for user in users:\n",
    "        for item in items:\n",
    "            x = [user, item, int(dummy_value)]\n",
    "            cust_test_data.append(x)\n",
    "            dummy_value = not(dummy_value)\n",
    "            \n",
    "    return np.array(cust_test_data)\n",
    "\n",
    "def predict(sess, args, model, users, items):\n",
    "    \n",
    "    cust_test_data = create_cust_test_data(users, items)   \n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(cust_test_data), args.batch_size):\n",
    "        feed_dict = get_feed_dict(args, model, cust_test_data, ripple_set, i, i + args.batch_size)\n",
    "        auc, acc, labels, batch_scores, predictions = model.custom_eval(sess, feed_dict)\n",
    "        scores = np.concatenate((scores, batch_scores))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_suggestion(user, k):\n",
    "    \n",
    "    items = [i for i in range(0, 2445)]\n",
    "    prediction = predict(sess, args, model, [user], items)\n",
    "    \n",
    "    recommend = [(prediction[i], i) for i in items]\n",
    "    recommend = sorted(recommend, reverse=True)[:k]\n",
    "    \n",
    "    return recommend\n",
    "#     return [x[1] for x in recommend]\n",
    "\n",
    "def get_top_truth(user, k):\n",
    "    if user not in truth_dict:\n",
    "        return []\n",
    "    return truth_dict[user][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_pred_truth(pred, truth, k):\n",
    "    pred_item_set = {x[1] for x in pred}\n",
    "    truth_item_set = set(truth)\n",
    "    \n",
    "    return pred_item_set.intersection(truth_item_set)\n",
    "\n",
    "def check_precision_at_k(sample_user, k):\n",
    "    \n",
    "    pred = get_top_suggestion(sample_user, k)\n",
    "    truth = get_top_truth(sample_user, k)\n",
    "    \n",
    "    intersect = get_intersect_pred_truth(pred, truth, k)\n",
    "    \n",
    "    if len(truth) > 0 :\n",
    "        return intersect, len(intersect) / len(truth)\n",
    "    else:\n",
    "        return {}, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 375/1000 [01:46<02:51,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occur for 1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:12<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "prec = []\n",
    "intersect = []\n",
    "\n",
    "for i in tqdm(range(1000, 2000)):\n",
    "    \n",
    "    try:\n",
    "        isec, p = check_precision_at_k(i, 10)\n",
    "    except:\n",
    "        p = 0\n",
    "        isec = {}\n",
    "        print(\"error occur for {}\".format(i))\n",
    "        \n",
    "    prec.append(p)\n",
    "    intersect.append(isec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11704484126984128"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check suggestion diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[206, 662, 787, 1635, 1669, 1784, 1816, 1975, 2082, 2314]\n",
      "[704, 706, 722, 730, 761, 795, 992, 1175, 1821, 2046]\n",
      "[12, 83, 145, 167, 220, 1556, 1614, 1637, 1678, 1978]\n",
      "[43, 471, 965, 1523, 1636, 1709, 1719, 1796, 1816, 2008]\n",
      "[144, 165, 309, 379, 975, 1021, 1124, 1554, 1776, 2252]\n",
      "[165, 309, 379, 670, 739, 748, 975, 1554, 1584, 1636]\n",
      "[379, 1225, 1669, 1816, 2082, 2151, 2154, 2185, 2314, 2422]\n",
      "[206, 662, 830, 1010, 1046, 1101, 1339, 1631, 1669, 1980]\n",
      "[478, 594, 738, 740, 742, 770, 771, 784, 803, 2149]\n",
      "\n",
      "intersect\n",
      "set() 0\n",
      "\n",
      "union\n",
      "{770, 771, 1796, 2314, 12, 784, 1554, 787, 1556, 1046, 1816, 795, 1821, 2082, 803, 43, 1584, 309, 1339, 830, 837, 1101, 1614, 594, 83, 1631, 1635, 1124, 1636, 1637, 2151, 2149, 2154, 2422, 379, 1669, 2185, 1678, 144, 145, 662, 1175, 670, 165, 167, 1709, 1975, 1719, 1978, 1980, 704, 706, 965, 1225, 2252, 206, 975, 722, 471, 2008, 730, 220, 478, 992, 738, 739, 740, 742, 748, 1776, 1010, 1523, 1784, 761, 1021, 2046} 76\n",
      "\n",
      "distinct rate\n",
      "0.76\n"
     ]
    }
   ],
   "source": [
    "offset = 0 # discard top n suggestion\n",
    "k = 10\n",
    "\n",
    "sample_user = [np.random.randint(1, 6000) for i in range(0, k)]\n",
    "\n",
    "intersect = {x[1] for x in get_top_suggestion(sample_user[0], k + offset)[offset:]}\n",
    "uni = intersect\n",
    "for i in range(1, 10):\n",
    "    s = {x[1] for x in get_top_suggestion(sample_user[i], k + offset)[offset:]}\n",
    "    print(sorted(s))\n",
    "    intersect = intersect.intersection(s)\n",
    "    uni = uni.union(s)\n",
    "    \n",
    "print(\"\\nintersect\")\n",
    "print(intersect, len(intersect))\n",
    "print(\"\\nunion\")\n",
    "print(uni, len(uni))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(uni)) / (10*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_user = [932, 12949, 11028, 4721, 4828, 8842, 2919, 1837, 6260, 2582]\n",
    "sample_user = [np.random.randint(1, 6000) for i in range(0, 10)]\n",
    "offset = 0\n",
    "sample_user = [i + offset for i in sample_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9949787022292842, 379),\n",
       " (0.9907776111330188, 1554),\n",
       " (0.9906589701924202, 1317),\n",
       " (0.9900562828823828, 670),\n",
       " (0.9893401738026825, 748),\n",
       " (0.9890906072650596, 309),\n",
       " (0.9884953753884527, 842),\n",
       " (0.9883920973200184, 165),\n",
       " (0.9869336351075841, 1448),\n",
       " (0.9857398754093053, 1584)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1171, 791, 1584, 829, 839, 213, 996, 231, 1259, 748]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({748, 1584}, 0.2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.999060215647246, 1877),\n",
       " (0.9975635347204143, 2112),\n",
       " (0.9960244407145277, 806),\n",
       " (0.9959166965965716, 1205),\n",
       " (0.9950955251584982, 2047),\n",
       " (0.9949919964234922, 801),\n",
       " (0.993860775846943, 1669),\n",
       " (0.9937174244263182, 309),\n",
       " (0.9935407686598229, 379),\n",
       " (0.9931039753694426, 738)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[748, 1357, 608, 1327]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(set(), 0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9994162547839369, 63),\n",
       " (0.9993687544440237, 149),\n",
       " (0.998149236281304, 335),\n",
       " (0.9976584089346259, 66),\n",
       " (0.9976229931899693, 159),\n",
       " (0.9966906675419538, 410),\n",
       " (0.996600063900833, 837),\n",
       " (0.9961186469468152, 237),\n",
       " (0.9960994811040359, 843),\n",
       " (0.9960407089398243, 1207)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1537, 6, 7, 1036, 1063, 2104, 82, 2135, 91]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(set(), 0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9959815770241308, 1669),\n",
       " (0.9942194209299215, 139),\n",
       " (0.9941449299549687, 206),\n",
       " (0.9939617906295704, 780),\n",
       " (0.9928173907229313, 1810),\n",
       " (0.992703663034211, 738),\n",
       " (0.9923489890941769, 739),\n",
       " (0.9922830189866911, 382),\n",
       " (0.991987056897304, 1687),\n",
       " (0.9914239044643746, 1554)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[769, 1667, 1669, 1419, 782, 1810, 1556, 1687, 541, 798]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({1669, 1687, 1810}, 0.3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.999712823084925, 986),\n",
       " (0.9994906455344359, 155),\n",
       " (0.9994372746028553, 830),\n",
       " (0.9993781402189208, 667),\n",
       " (0.9991432737800903, 662),\n",
       " (0.9990595481015948, 1010),\n",
       " (0.999000685755902, 364),\n",
       " (0.9988491841728675, 344),\n",
       " (0.9987188448948912, 189),\n",
       " (0.9986912892876257, 1257)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1024, 768, 1027, 1416, 1294, 1166, 1169, 791, 1177, 1178]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(set(), 0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9958944889875023, 2252),\n",
       " (0.9935883202929446, 1021),\n",
       " (0.9927315279118385, 1885),\n",
       " (0.9899252115017091, 1913),\n",
       " (0.9895268314588828, 1244),\n",
       " (0.9895044660692416, 422),\n",
       " (0.9893905727814648, 938),\n",
       " (0.9890818180261994, 1524),\n",
       " (0.9881705963361966, 1531),\n",
       " (0.9877554845541872, 838)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[780, 1530, 1425, 1042, 534, 159, 165, 1836, 2221, 688]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(set(), 0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9905442462952411, 854),\n",
       " (0.9870971353456817, 1215),\n",
       " (0.9861570733585657, 1976),\n",
       " (0.9844411189002364, 222),\n",
       " (0.9829729926149515, 394),\n",
       " (0.9824850589036244, 829),\n",
       " (0.9823330245476719, 1244),\n",
       " (0.9821221124038972, 924),\n",
       " (0.980544541135069, 1688),\n",
       " (0.9802304532321771, 1210)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1539, 517, 1030, 7, 1554, 534, 1561, 2074, 26]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(set(), 0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9971291573923741, 1288),\n",
       " (0.9970250747634652, 1636),\n",
       " (0.9938505693867882, 26),\n",
       " (0.9936477261682949, 1981),\n",
       " (0.9929604990408918, 139),\n",
       " (0.9929522841044582, 2082),\n",
       " (0.9925235935291208, 1509),\n",
       " (0.9923332682564275, 1629),\n",
       " (0.9922887199192714, 1719),\n",
       " (0.992105828392345, 856)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1926, 1288, 2186, 16, 18, 1555, 1561, 922, 1068, 1974]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({1288}, 0.1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9979623366108673, 2248),\n",
       " (0.9886342532374198, 309),\n",
       " (0.9882869820983248, 379),\n",
       " (0.9831093326547107, 1776),\n",
       " (0.9756445864089578, 2165),\n",
       " (0.9754228154853846, 748),\n",
       " (0.9750524358403163, 1205),\n",
       " (0.9706008257614086, 670),\n",
       " (0.9702546703408239, 1554),\n",
       " (0.9683744951543785, 165)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1554, 1735, 2248, 850, 1880, 738, 740, 1774, 1776, 627]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({1554, 1776, 2248}, 0.3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9771795813572126, 379),\n",
       " (0.9671166932335017, 738),\n",
       " (0.966336421107506, 1554),\n",
       " (0.9660846269076441, 670),\n",
       " (0.9658817993348593, 309),\n",
       " (0.9653243978888548, 165),\n",
       " (0.9626608781111219, 1669),\n",
       " (0.9604584063675919, 780),\n",
       " (0.9551215513195715, 748),\n",
       " (0.954859537948739, 1225)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[670, 578, 1225, 2125, 594, 1493, 1880, 738, 740, 755]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({670, 738, 1225}, 0.3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for user in sample_user:\n",
    "    print(user)\n",
    "    \n",
    "    prediction = get_top_suggestion(user, 10)\n",
    "    truth = get_top_truth(user, 10)\n",
    "    \n",
    "    display((prediction))\n",
    "    display((truth))\n",
    "    display(check_precision_at_k(user, 10))\n",
    "    display(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
